{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2\n",
    "\n",
    "Large Language Models (LLMs) can be categorized into different structures based on their architecture:\n",
    "\n",
    "1. **Encoder-Only Models**: These models, like BERT, are optimized for understanding and generating embeddings from input text. They are often used for tasks like classification and named entity recognition.\n",
    "\n",
    "2. **Decoder-Only Models**: These models, like GPT-2, are designed for text generation. They predict the next word in a sequence, making them ideal for tasks such as text completion and translation.\n",
    "\n",
    "3. **Encoder-Decoder Models**: These models, like T5, combine both encoder and decoder components. They are versatile and can be used for a wide range of tasks, including text generation and comprehension.\n",
    "To add an image to a markdown cell in Jupyter Notebook, you can use the following syntax:\n",
    "\n",
    "\n",
    "![](./Images/Vision-Transformer-Model-Architecture-1024x746.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first question is how we can feed a prompt to a model?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is evident in the overall structure of a transformer model, we first need to convert the input text into a format that the model can understand. This conversion process, called Tokenization, involves transforming the text into numerical representations. \n",
    "\n",
    "Tokenization involves dividing text into smaller, more manageable units known as tokens and assigning each a unique integer identifier. This is usually done with specialized tokenizers. These tokens form a vocabulary, where each token is mapped to a specific integer.\n",
    "\n",
    "Advanced tokenization methods surpass basic word and whitespace splitting, using more complex techniques to create tokens, often leading to an extensive vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
